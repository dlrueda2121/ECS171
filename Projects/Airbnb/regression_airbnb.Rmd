---
title: "regression_airbnb"
author: "Diana Rueda"
date: "4/29/2021"
output:
  pdf_document: default
  html_document: default
---




```{r}
#Loading and showing the head of the dataset containing dummy variables
library(readr)
df_dummies <- read_csv("airbnb_dummies.csv")
df_dummies <- df_dummies[,!(names(df_dummies) %in% c("X1"))]
head(df_dummies)
```


Building model with all variables to observe behavior

```{r}
model <- lm(price ~ ., data = df_dummies)
summary(model)
```


```{r}
# diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 
plot(model)
```

Note: the dummy variables won't require all columns to be in the model since the last one will be a linear combination of the rest. IF we include 2 out of 3 dummies for example in room_type that will have carry the complete information from this variable. The correlation within dummy variables is what causes the NA values in the model above.     
      
      
The followig library performs diagnostics for different models in linear regression. It adds one variable at a time to evaluate which model has better performance. We imput a model with all the possible variables and it gives us a final model. 

```{r}
# Stepwise Regression
library(MASS)
fit <- lm(price ~ ., data = df_dummies)
step <- stepAIC(fit, direction="both")
step$anova # display results
```


To compare the three models in the process above I built the three model and placed them side to side in anova tables. This is not necessary at all but it helped me understand the difference of one to the other. Note that the number shown in Sum of Sq is the difference between the models and we prefer a lower RSS.

```{r}
# compare models
fit2 <- lm(price ~ minimum_nights + number_of_reviews + availability_365 + 
    neighbourhood_group_Brooklyn + neighbourhood_group_Manhattan + 
    neighbourhood_group_Queens + `room_type_Entire home/apt` + 
    `room_type_Private room`, data = df_dummies)

```


I used bootstrap to perform 10 fold crossvalidation for R-squared. 


```{r}
# Assessing R2 shrinkage using 10-Fold Cross-Validation 

fit <- lm(price ~ minimum_nights + number_of_reviews + availability_365 + 
    neighbourhood_group_Brooklyn + neighbourhood_group_Manhattan + 
    neighbourhood_group_Queens + `room_type_Entire home/apt` + 
    `room_type_Private room`, data = df_dummies) 

library(bootstrap)
# define functions 
theta.fit <- function(x,y){lsfit(x,y)}
theta.predict <- function(fit,x){cbind(1,x)%*%fit$coef} 

# matrix of predictors
X <- as.matrix(df_dummies[c("availability_365","number_of_reviews", "minimum_nights", "neighbourhood_group_Brooklyn", "neighbourhood_group_Manhattan", "neighbourhood_group_Queens", "room_type_Entire home/apt", "room_type_Private room")])
# vector of predicted values
y <- as.matrix(df_dummies[c("price")]) 

results <- crossval(X,y,theta.fit,theta.predict,ngroup=10)
cor(y, fit$fitted.values)**2 # raw R2 
cor(y, results$cv.fit)**2 # cross-validated R2
```


This is an interesing plot because we get to see which variables have relatively more importance across the board. 

```{r}
# Calculate Relative Importance for Each Predictor
library(relaimpo)
calc.relimp(fit,type=c("lmg","last","first","pratt"),
   rela=TRUE)

# Bootstrap Measures of Relative Importance (1000 samples) 
boot <- boot.relimp(fit, b = 1000, type = c("lmg", 
  "last", "first", "pratt"), rank = TRUE, 
  diff = TRUE, rela = TRUE)
booteval.relimp(boot) # print result
plot(booteval.relimp(boot,sort=TRUE)) # plot result
```

---------------------------------------------

Below are the individual detailed anova tables of all three model, but note that we have chosen the model:
lm(formula = price ~ minimum_nights + number_of_reviews + availability_365 + 
    neighbourhood_group_Bronx + neighbourhood_group_Brooklyn + 
    neighbourhood_group_Manhattan + neighbourhood_group_Queens + 
    neighbourhood_group_Staten Island + room_type_Entire home/apt + 
    room_type_Private room + room_type_Shared room, data = df_dummies)
    
for our analysis. For the purpose of identification it goes by "fit" and "fit2"

```{r}
summary(fit2)


```

The following are the confidence intervals for the estimates of our regression model. Note that none of them contain zero which is good for us. We have chosen good model.

```{r}
confint(fit2, level=0.95)
```


The following two plots dont look great which is not encouraging but they are not the worst?

```{r}
plot(fit2$residuals)
```

```{r}
# diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 

plot(fit2)

```

---------------------------------

In the following section of my code I perform hypothesis testing for the model parameters through multivariable statistical analysis.

```{r, echo = FALSE}
#Setting up the model
Y <- as.matrix(df_dummies[,1])
n <- length(Y)
Z <- cbind(rep(1,n),as.matrix(df_dummies[,c("minimum_nights", "number_of_reviews", "availability_365" ,
    "neighbourhood_group_Brooklyn", "neighbourhood_group_Manhattan" ,
    "neighbourhood_group_Queens", "room_type_Entire home/apt" ,
    "room_type_Private room")]))
r <- dim(Z)[2]-1
model = lm(Y~Z)
summary_first = summary(model)

```


Parameter estimation 

```{r}
# least square estimates
beta_hat <- solve(t(Z)%*%Z)%*%t(Z)%*%Y
beta_hat
```

```{r}
# R^2 statistic
R_square <- 1 - sum((Y - Z%*%beta_hat)^2)/sum((Y-mean(Y))^2)
R_square
```

```{r}
# sigma_hat_square
sigma_hat_square <- sum((Y - Z%*%beta_hat)^2)/(n-r-1)
sigma_hat_square
```

```{r}
# estimated covariance of hat{beta}
#not to use but I printed to confirm
cov_est = sigma_hat_square * solve(t(Z)%*%Z)
cov_est
```

```{r}
Omega <- solve(t(Z)%*%Z)
```


##### Hypothesis testing
We want to reject the hypothesis H_0: beta_j = 0, for this we need a t-statistic (t_stat) larger than a critical value (cval_t). Unfortunatly we fail to reject the hypothesis for the first two betas which is not enough evidence to eliminate them but it does not support the integrity of out model positively.

```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 1
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_1$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can not reject the null hypothesis that the estimate $\hat{\beta}_1$ is zero at level alpha = 0.05 .

```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 2
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_2$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can not reject the null hypothesis that the estimate $\hat{\beta}_2$ is zero at level alpha = 0.05 .

```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 3
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_3$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can reject the null hypothesis that the estimate $\hat{\beta}_3$ is zero at level alpha = 0.05 .

```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 4
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_4$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can reject the null hypothesis that the estimate $\hat{\beta}_4$ is zero at level alpha = 0.05 .


```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 5
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_5$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can reject the null hypothesis that the estimate $\hat{\beta}_5$ is zero at level alpha = 0.05 .

```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 6
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_6$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can reject the null hypothesis that the estimate $\hat{\beta}_6$ is zero at level alpha = 0.05 .


```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 7
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_7$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can reject the null hypothesis that the estimate $\hat{\beta}_7$ is zero at level alpha = 0.05 .

```{r}
# t-test for single coefficient
# H_0: beta_j = 0, H_a: beta_j != 0

j <- 8
t_stat <- (beta_hat[j+1] - 0)/sqrt(sigma_hat_square * solve(t(Z)%*%Z)[j+1,j+1])

alpha <- 0.05
cval_t <- qt(1-alpha/2, n-r-1)
t_stat
cval_t
```

From t-test for $\hat{\beta}_8$ we obtained a t-statistic of `r t_stat` and a `r cval_t`    
Therefore we can reject the null hypothesis that the estimate $\hat{\beta}_8$ is zero at level alpha = 0.05 .


---------------------------
 
The following is an F-test to test for the hypothesis H_0: beta_1 = beta_2 = ... = beta_8 = 0. A result in our favor here will help support the integrity of our model.

```{r}
# F-test
# H_0: beta_1 = ...beta_8 = 0
C <- matrix(c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1),8,9)

df_1 <- qr(C)$rank # df_1: rank of matrix C
q = 0

Omega_22 = C%*% solve(t(Z)%*%Z) %*%t(C)
f_stat <- t(C%*%beta_hat)%*%solve(Omega_22)%*%(C%*%beta_hat)

cval_f <- qf(1-alpha, 2, n-r-1)
critical = cval_f * df_1 * sigma_hat_square
f_stat
critical
```

with a level alpha = 0.05 we find that the f-statistic is `r f_stat` while the critical value is `r critical`. Therefore we can reject the null hypothesis that the estimates $\hat{\beta}_1 = \hat{\beta}_2 = ... = \hat{\beta}_8 = 0$ at level alpha = 0.05 . 


```{r}
#These are here again only for easy access to comparison 
confint(fit2, level=0.95)
```


------------------------------------------

```{r}
df_bronx = df_dummies[df_dummies$neighbourhood_group_Bronx == 1,]
head(df_bronx)
```



```{r}
fit_bronx <- lm(price ~ ., data = df_bronx)
summary(fit_bronx)

```


```{r}
# Stepwise Regression
library(MASS)
step <- stepAIC(fit_bronx, direction="both")
step$anova # display results
```


```{r}
fit_bronx <- lm(price ~ minimum_nights + number_of_reviews + availability_365 + 
    `room_type_Entire home/apt`, data = df_bronx)
summary(fit_bronx)
```













